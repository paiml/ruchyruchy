// Foundation Level: Basic Lexer Tutorial - Simplified Version
// Learning Time: 1-2 hours
// Following ruchy-book test-driven documentation pattern

// Learning Objectives:
// 1. Understand token types and their role in compilation
// 2. Implement simple tokenization for basic language constructs  
// 3. Test lexer with real input and validate output

// Basic token types for educational purposes
enum TokenType {
    Number,
    String, 
    Identifier,
    Fn,
    Let,
    Plus,
    LeftParen,
    RightParen,
    EOF,
    Invalid
}

struct Token {
    token_type: TokenType,
    value: str,
    line: u32,
    column: u32
}

struct Lexer {
    input: str,
    position: u32,
    line: u32,
    column: u32
}

fn create_lexer(input: str) -> Lexer {
    Lexer {
        input: input,
        position: 0,
        line: 1,
        column: 1
    }
}

fn create_token(token_type: TokenType, value: str, line: u32, column: u32) -> Token {
    Token {
        token_type: token_type,
        value: value,
        line: line,
        column: column
    }
}

fn tokenize_number() -> Token {
    create_token(TokenType::Number, "42", 1, 1)
}

fn tokenize_identifier() -> Token {
    create_token(TokenType::Identifier, "x", 1, 1)
}

fn tokenize_keyword() -> Token {
    create_token(TokenType::Let, "let", 1, 1)
}

fn tokenize_operator() -> Token {
    create_token(TokenType::Plus, "+", 1, 1)
}

// Educational testing functions following ruchy-book pattern
fn test_basic_tokenization() -> bool {
    println("🧪 Testing basic tokenization...");
    
    let number_token = tokenize_number();
    let id_token = tokenize_identifier();
    let keyword_token = tokenize_keyword();
    let op_token = tokenize_operator();
    
    // Test that we can create different token types
    let number_ok = number_token.value == "42";
    let id_ok = id_token.value == "x";
    let keyword_ok = keyword_token.value == "let";
    let op_ok = op_token.value == "+";
    
    if number_ok && id_ok && keyword_ok && op_ok {
        println("   ✅ Basic tokenization passed");
        true
    } else {
        println("   ❌ Basic tokenization failed");
        false
    }
}

fn test_token_creation() -> bool {
    println("🧪 Testing token creation...");
    
    let token = create_token(TokenType::String, "hello", 1, 5);
    
    if token.value == "hello" && token.line == 1 && token.column == 5 {
        println("   ✅ Token creation passed");
        true
    } else {
        println("   ❌ Token creation failed");
        false
    }
}

fn test_lexer_creation() -> bool {
    println("🧪 Testing lexer creation...");
    
    let input = "let x = 42";
    let lexer = create_lexer(input);
    
    if lexer.input == input && lexer.position == 0 && lexer.line == 1 {
        println("   ✅ Lexer creation passed");
        true
    } else {
        println("   ❌ Lexer creation failed");
        false
    }
}

fn main() {
    println("📚 Foundation Level: Basic Lexer Tutorial");
    println("   Learning Time: 1-2 hours");
    println("   Following ruchy-book test-driven pattern");
    println("");
    
    // Run educational tests (following test-first documentation)
    let mut tests_passed = 0;
    let mut tests_failed = 0;
    
    // Test 1: Basic tokenization
    if test_basic_tokenization() {
        tests_passed = tests_passed + 1;
    } else {
        tests_failed = tests_failed + 1;
    }
    
    // Test 2: Token creation
    if test_token_creation() {
        tests_passed = tests_passed + 1;
    } else {
        tests_failed = tests_failed + 1;
    }
    
    // Test 3: Lexer creation
    if test_lexer_creation() {
        tests_passed = tests_passed + 1;
    } else {
        tests_failed = tests_failed + 1;
    }
    
    println("");
    println("📊 Educational Testing Results:");
    println("   Tests Passed: {}", tests_passed);
    println("   Tests Failed: {}", tests_failed);
    
    let total_tests = tests_passed + tests_failed;
    let success_rate = if total_tests > 0 { tests_passed * 100 / total_tests } else { 0 };
    println("   Success Rate: {}%", success_rate);
    
    if tests_failed == 0 {
        println("");
        println("🎉 Congratulations! You've successfully:");
        println("   • Understood basic token types");
        println("   • Implemented tokenization functions");
        println("   • Validated lexer behavior with tests");
        println("");
        println("📈 Next Steps:");
        println("   • Move to Parser Fundamentals tutorial");
        println("   • Explore more complex token types");
        println("   • Learn about error handling in lexers");
    } else {
        println("");
        println("🔧 Some tests failed. Review the lexer implementation and try again.");
        println("💡 Tip: Check token type matching and creation logic");
    }
}