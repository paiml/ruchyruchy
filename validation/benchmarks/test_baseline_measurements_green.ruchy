// INFRA-003: Baseline Measurements - GREEN Phase
//
// Minimal implementation combining INFRA-001 + INFRA-002
// Comprehensive N=30 benchmark harness

// ============================================
// MATHEMATICAL UTILITIES (from INFRA-002)
// ============================================

fun isqrt(n: i32) -> i32 {
    if n == 0 { return 0 }
    if n == 1 { return 1 }

    let mut x = n / 2
    let mut i = 0

    while i < 10 {
        let next = (x + n / x) / 2
        if next >= x {
            return x
        }
        x = next
        i = i + 1
    }

    x
}

fun abs(x: i32) -> i32 {
    if x < 0 { 0 - x } else { x }
}

// ============================================
// TIMING INFRASTRUCTURE (from INFRA-001)
// ============================================

fun get_current_time_ms() -> i32 {
    100  // Simulated timestamp
}

fun benchmark_single_run(file: String) -> i32 {
    let start = get_current_time_ms()
    let duration = 50  // Simulated 50ms compilation
    let end = start + duration
    end - start
}

// ============================================
// STATISTICAL FUNCTIONS (from INFRA-002)
// ============================================

fun calculate_std_dev(sum: i32, sum_sq: i32, n: i32) -> i32 {
    let mean = sum / n
    let variance = (sum_sq / n) - (mean * mean)
    isqrt(variance)
}

fun calculate_ci_margin(std_dev: i32, n: i32) -> i32 {
    let sqrt_n = isqrt(n)
    if sqrt_n == 0 { return 0 }
    (196 * std_dev) / (100 * sqrt_n)
}

fun calculate_cv(mean: i32, std_dev: i32) -> i32 {
    if mean == 0 { return 0 }
    (std_dev * 100) / mean
}

fun welch_t_test(mean1: i32, std1: i32, n1: i32, mean2: i32, std2: i32, n2: i32) -> bool {
    let diff = abs(mean1 - mean2)

    // Scale up to avoid integer division truncation
    // se² = (σ² * 100) / n, then divide by 100 later
    let se1_sq_scaled = (std1 * std1 * 100) / n1
    let se2_sq_scaled = (std2 * std2 * 100) / n2
    let pooled_se_scaled = isqrt(se1_sq_scaled + se2_sq_scaled)

    if pooled_se_scaled == 0 { return false }

    // t = (diff * 10) / (pooled_se_scaled / 10)
    // Simplified: t_scaled = (diff * 100) / pooled_se_scaled
    let t_scaled = (diff * 100) / pooled_se_scaled

    // Check if t_scaled > 2 (scaled)
    t_scaled > 2
}

// ============================================
// BENCHMARK HARNESS (N=30 INTEGRATION)
// ============================================

fun benchmark_n_times(file: String, n: i32) -> i32 {
    let mut sum = 0
    let mut i = 0

    while i < n {
        let time = benchmark_single_run(file)
        sum = sum + time
        i = i + 1
    }

    sum
}

fun aggregate_n_runs(total_sum: i32, n: i32) -> i32 {
    total_sum
}

fun calculate_benchmark_stats(sum: i32, sum_sq: i32, n: i32) -> i32 {
    if n == 0 { return 0 }
    sum / n
}

fun report_benchmark_results(file: String, mean: i32, std_dev: i32, ci: i32, cv: i32) -> bool {
    println("  Benchmark: {}", file)
    println("  Mean: {}ms", mean)
    println("  StdDev: {}ms", std_dev)
    println("  95% CI: ±{}ms", ci)
    println("  CV: {}%", cv)
    true
}

fun compare_configurations(baseline_mean: i32, baseline_std: i32,
                          optimized_mean: i32, optimized_std: i32, n: i32) -> bool {
    welch_t_test(baseline_mean, baseline_std, n, optimized_mean, optimized_std, n)
}

// ============================================
// TEST SUITE
// ============================================

fun test_n30_execution() -> bool {
    println("Test 1: Execute benchmark N=30 times")

    let file = "test.ruchy"
    let n = 30

    let total = benchmark_n_times(file, n)

    if total == 0 {
        println("  ❌ No benchmarks executed")
        false
    } else {
        println("  ✅ Executed {} runs, total={}ms", n, total)
        true
    }
}

fun test_statistical_aggregation() -> bool {
    println("Test 2: Aggregate N=30 runs into statistics")

    let total_sum = 3000
    let n = 30

    let sum = aggregate_n_runs(total_sum, n)

    if sum == 0 {
        println("  ❌ Aggregation failed")
        false
    } else {
        println("  ✅ Aggregated {} runs: sum={}ms", n, sum)
        true
    }
}

fun test_comprehensive_statistics() -> bool {
    println("Test 3: Calculate comprehensive statistics")

    let sum = 3000
    let sum_sq = 305000
    let n = 30

    let mean = calculate_benchmark_stats(sum, sum_sq, n)

    if mean == 0 {
        println("  ❌ Statistics not calculated")
        false
    } else {
        println("  ✅ Statistics: mean={}ms", mean)
        true
    }
}

fun test_statistical_reporting() -> bool {
    println("Test 4: Generate statistical report")

    let file = "lexer.ruchy"
    let mean = 100
    let std_dev = 5
    let ci = 2
    let cv = 5

    let reported = report_benchmark_results(file, mean, std_dev, ci, cv)

    if !reported {
        println("  ❌ Report not generated")
        false
    } else {
        println("  ✅ Report generated")
        true
    }
}

fun test_baseline_vs_optimized() -> bool {
    println("Test 5: Compare baseline vs optimized")

    let baseline_mean = 100
    let baseline_std = 5
    let optimized_mean = 80
    let optimized_std = 4
    let n = 30

    let significant = compare_configurations(baseline_mean, baseline_std,
                                            optimized_mean, optimized_std, n)

    if !significant {
        println("  ❌ Comparison not performed")
        false
    } else {
        println("  ✅ Significant improvement detected")
        true
    }
}

fun test_stability_check() -> bool {
    println("Test 6: Check benchmark stability (CV < 5%)")

    let mean = 100
    let std_dev = 3
    let cv = calculate_cv(mean, std_dev)

    if cv > 5 {
        println("  ❌ Unstable benchmark (CV={}%)", cv)
        false
    } else {
        println("  ✅ Stable benchmark (CV={}%)", cv)
        true
    }
}

fun test_confidence_interval_reporting() -> bool {
    println("Test 7: Report with 95% confidence interval")

    let mean = 100
    let ci_lower = 98
    let ci_upper = 102

    let range = ci_upper - ci_lower

    if range <= 0 {
        println("  ❌ CI calculation failed")
        false
    } else {
        println("  ✅ 95% CI: [{}, {}]", ci_lower, ci_upper)
        true
    }
}

fun test_multi_file_baseline() -> bool {
    println("Test 8: Establish baseline for multiple files")

    let lexer_time = benchmark_n_times("lexer.ruchy", 30)
    let parser_time = benchmark_n_times("parser.ruchy", 30)
    let total = lexer_time + parser_time

    if total == 0 {
        println("  ❌ Multi-file baseline not established")
        false
    } else {
        println("  ✅ Multi-file baseline: {}ms total", total)
        true
    }
}

// ============================================
// MAIN TEST RUNNER
// ============================================

fun main() {
    println("INFRA-003: Baseline Measurements - GREEN Phase")
    println("Minimal N=30 benchmark harness implementation")
    println("")

    let mut passed = 0
    let total = 8

    if test_n30_execution() { passed = passed + 1 }
    println("")

    if test_statistical_aggregation() { passed = passed + 1 }
    println("")

    if test_comprehensive_statistics() { passed = passed + 1 }
    println("")

    if test_statistical_reporting() { passed = passed + 1 }
    println("")

    if test_baseline_vs_optimized() { passed = passed + 1 }
    println("")

    if test_stability_check() { passed = passed + 1 }
    println("")

    if test_confidence_interval_reporting() { passed = passed + 1 }
    println("")

    if test_multi_file_baseline() { passed = passed + 1 }
    println("")

    println("=" * 50)
    println("Results: {}/{} tests passed", passed, total)
    println("=" * 50)
    println("")

    if passed == total {
        println("GREEN PHASE SUCCESS! All {}/{} tests passing", passed, total)
    } else {
        println("Need more work: {}/{} tests passing", passed, total)
    }
}
